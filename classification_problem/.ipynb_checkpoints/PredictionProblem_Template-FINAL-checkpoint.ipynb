{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5dbbea0e-3a99-459e-abd6-0959e6946869",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem\"\n",
    "subtitle: \"STAT 303-3 Spring 25\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    code-fold: show\n",
    "    embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7523b-6099-4001-a997-60005980e73a",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "- Put the parts of your code under the corresponding sections. (0.25/2 points will be taken off for not doing this.)\n",
    "- Do not include any redundant/irrelevant code, text or comments. (0.5/2 points will be taken off for not doing this.)\n",
    "- **Your code must run without any errors or runtime issues.** (Failure to meet this condition will result in a 0.)\n",
    "- **Your code must return your Public Leaderboard score.** (Failure to meet this condition will result in a 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03031cca-1a24-4e66-926e-2b32f85f3cc7",
   "metadata": {},
   "source": [
    "## 1) Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65f439-927b-4b40-9008-a715356fbe58",
   "metadata": {},
   "source": [
    "Put all the Python libraries and tools you imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb866b1-8f62-4ca4-a39e-5774486da357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "# sklearn tools\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics, impute\n",
    "from sklearn.impute import KNNImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877d116-1e8d-426e-84ba-10c47e28a1ac",
   "metadata": {},
   "source": [
    "## 2) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177363af-51a7-4b16-b6d2-33ba5796bd20",
   "metadata": {},
   "source": [
    "- This section is required to include the code that reads, cleans and preprocesses the datasets.\n",
    "- Note that both the training and test datasets should undergo the same sequence of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcc4a5b-777a-4b75-8fd7-b0594591ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/yd_rkzv93459x04g3jzsjwcr0000gn/T/ipykernel_40708/722514571.py:43: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['host_identity_verified'] = train['host_identity_verified'].fillna(False).astype(int)\n",
      "/var/folders/yg/yd_rkzv93459x04g3jzsjwcr0000gn/T/ipykernel_40708/722514571.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['host_identity_verified'] = test['host_identity_verified'].fillna(False).astype(int)\n",
      "/var/folders/yg/yd_rkzv93459x04g3jzsjwcr0000gn/T/ipykernel_40708/722514571.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train['host_has_profile_pic'] = train['host_has_profile_pic'].fillna(False).astype(int)\n",
      "/var/folders/yg/yd_rkzv93459x04g3jzsjwcr0000gn/T/ipykernel_40708/722514571.py:47: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test['host_has_profile_pic'] = test['host_has_profile_pic'].fillna(False).astype(int)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# initial cols to drop\n",
    "\n",
    "cols_to_drop = ['has_availability', 'host_neighbourhood', 'neighbourhood_cleansed', 'first_review',\n",
    "               'last_review', 'room_type', 'host_response_time', 'host_location', 'host_about']\n",
    "\n",
    "train = train.drop(columns = cols_to_drop)\n",
    "test = test.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "# clean numeric vals with symbols\n",
    "\n",
    "def clean_cols(val):\n",
    "    if isinstance(val, str):\n",
    "        val = val.replace('$', '').replace('%', '').replace(',', '')\n",
    "    return float(val)\n",
    "\n",
    "train[['host_acceptance_rate','host_response_rate']] = train[['host_acceptance_rate','host_response_rate']].map(clean_cols)\n",
    "test[['host_acceptance_rate','host_response_rate']] = test[['host_acceptance_rate','host_response_rate']].map(clean_cols)\n",
    "\n",
    "\n",
    "# clean bathrooms_text - convert half-bath to 0.5\n",
    "train['bathrooms_text'] = train['bathrooms_text'].replace('Half-bath', '0.5')\n",
    "test['bathrooms_text'] = test['bathrooms_text'].replace('Half-bath', '0.5')\n",
    "\n",
    "# extract numeric part\n",
    "train['bathrooms_text'] = train['bathrooms_text'].str.extract(r'(\\d+\\.?\\d*)')[0].astype(float)\n",
    "test['bathrooms_text'] = test['bathrooms_text'].str.extract(r'(\\d+\\.?\\d*)')[0].astype(float)\n",
    "\n",
    "test = test.rename(columns={'bathrooms_text': 'bathrooms'})\n",
    "train = train.rename(columns={'bathrooms_text': 'bathrooms'})\n",
    "\n",
    "\n",
    "# host identity verified - convert to 0/1\n",
    "# instant bookable - convert to 0/1\n",
    "# host profile pic - convert to 0/1\n",
    "\n",
    "train['instant_bookable'] = train['instant_bookable'].fillna(False).astype(int)\n",
    "test['instant_bookable'] = test['instant_bookable'].fillna(False).astype(int)\n",
    "\n",
    "train['host_identity_verified'] = train['host_identity_verified'].fillna(False).astype(int)\n",
    "test['host_identity_verified'] = test['host_identity_verified'].fillna(False).astype(int)\n",
    "\n",
    "train['host_has_profile_pic'] = train['host_has_profile_pic'].fillna(False).astype(int)\n",
    "test['host_has_profile_pic'] = test['host_has_profile_pic'].fillna(False).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# property type -- make new col, 1 if entire property and 0 otherwise\n",
    "\n",
    "train['is_entire_place'] = train['property_type'].str.contains('entire', case=False, na=False).astype(int)\n",
    "test['is_entire_place'] = test['property_type'].str.contains('entire', case=False, na=False).astype(int)\n",
    "\n",
    "train = train.drop('property_type', axis=1)\n",
    "test = test.drop('property_type', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# description -- make new col, 1 if contains 'luxury' words\n",
    "\n",
    "keywords = ['luxury', 'luxurious', 'penthouse', 'exclusive', 'elegant',\n",
    "            'premium', 'high-end', 'designer', 'upscale', 'chic',\n",
    "            'modern', 'deluxe', 'sophisticated', 'breathtaking','custom-built', 'architect-designed',\n",
    "            'state-of-the-art','prestigious', 'top-tier', '5-star', 'five-star']\n",
    "\n",
    "pattern = '|'.join(keywords)\n",
    "train['luxury_description'] = train['description'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "test['luxury_description'] = test['description'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "\n",
    "train = train.drop('description', axis=1)\n",
    "test = test.drop('description', axis=1)\n",
    "\n",
    "\n",
    "# OHE location\n",
    "\n",
    "location_dummies = pd.get_dummies(train['listing_location'], prefix='location', drop_first=True)\n",
    "train = pd.concat([train, location_dummies], axis=1).drop('listing_location', axis=1)\n",
    "\n",
    "location_dummies = pd.get_dummies(test['listing_location'], prefix='location', drop_first=True)\n",
    "test = pd.concat([test, location_dummies], axis=1).drop('listing_location', axis=1)\n",
    "\n",
    "train[['location_chicago', 'location_kauai']] = train[['location_chicago', 'location_kauai']].astype(int)\n",
    "test[['location_chicago', 'location_kauai']] = test[['location_chicago', 'location_kauai']].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# host_verifications, amenities. -- convert to length\n",
    "\n",
    "def make_len(val):\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return np.nan\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return np.nan\n",
    "    return len(val)\n",
    "    \n",
    "\n",
    "train['host_verifications'] = train['host_verifications'].map(make_len)\n",
    "train['amenities'] = train['amenities'].map(make_len)\n",
    "\n",
    "test['host_verifications'] = test['host_verifications'].map(make_len)\n",
    "test['amenities'] = test['amenities'].map(make_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ecb3d9-8268-4a3e-bd8d-e03b2442f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = pd.to_datetime(\"today\")\n",
    "train['host_since'] = pd.to_datetime(train['host_since'], errors='coerce')\n",
    "train['host_since'] = (today - train['host_since']).dt.days // 365\n",
    "\n",
    "test['host_since'] = pd.to_datetime(test['host_since'], errors='coerce')\n",
    "test['host_since'] = (today - test['host_since']).dt.days // 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55349781-a31e-4632-a47f-99f3d6ceb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTE MISSING VALS\n",
    "\n",
    "# fit to train data, transform train and test data\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "train_impute_cols = [col for col in train.columns if col not in ['id', 'host_is_superhost']]\n",
    "\n",
    "#scale\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_array = scaler.fit_transform(train[train_impute_cols])\n",
    "scaled_train_df = pd.DataFrame(scaled_train_array, columns=train_impute_cols, index=train.index)\n",
    "\n",
    "train_imputed_arr = imputer.fit_transform(scaled_train_df) # first scale\n",
    "train_unscaled_data = scaler.inverse_transform(train_imputed_arr) # then undo scale\n",
    "\n",
    "train_imputed = train.copy()\n",
    "train_imputed[train_impute_cols] = pd.DataFrame(train_unscaled_data, columns=train_impute_cols, index=train.index)\n",
    "\n",
    "\n",
    "# test - use same scale\n",
    "test_impute_cols = [col for col in test.columns if col != 'id']\n",
    "\n",
    "\n",
    "scaled_test_array = scaler.transform(test[test_impute_cols])\n",
    "scaled_test_df = pd.DataFrame(scaled_test_array, columns=test_impute_cols, index=test.index)\n",
    "\n",
    "test_imputed_arr = imputer.transform(scaled_test_df)\n",
    "test_unscaled_data = scaler.inverse_transform(test_imputed_arr)\n",
    "\n",
    "test_imputed = test.copy()\n",
    "test_imputed[test_impute_cols] = pd.DataFrame(test_unscaled_data, columns = test_impute_cols, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38569f5-a5e8-4330-b2ba-780e4fab13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_imputed.drop(columns=['id', 'host_is_superhost'], axis=1)\n",
    "\n",
    "# Adding constant: necessary for the calculations\n",
    "X_const = add_constant(X)\n",
    "\n",
    "vif_data = pd.DataFrame() # empty df\n",
    "vif_data['variable'] = X_const.columns # Put the names in the first col\n",
    "\n",
    "for i in range(len(X_const.columns)):\n",
    "    vif_data.loc[i,'VIF'] = variance_inflation_factor(X_const.values, i)\n",
    "\n",
    "vif_data\n",
    "\n",
    "\n",
    "# minimum_nights, maximum_minimum_nights have VIF scores over 7\n",
    "\n",
    "train_imputed = train_imputed.drop(columns=['minimum_nights','maximum_minimum_nights'])\n",
    "test_imputed = test_imputed.drop(columns=['minimum_nights','maximum_minimum_nights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6e772a-c261-4fcc-b6e4-fc749b069079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice predictors - no id !! but save for predictions\n",
    "\n",
    "X_train = train_imputed.drop(columns=['host_is_superhost','id'])\n",
    "y_train = train_imputed['host_is_superhost']\n",
    "\n",
    "X_test = test_imputed.drop(columns=['id'])\n",
    "test_ids = test_imputed['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078e4fa-9a22-44ca-b1e3-944f25dac70a",
   "metadata": {},
   "source": [
    "## 3) Machine Learning Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5c8b4-9aab-4022-8561-5f8bf964cab3",
   "metadata": {},
   "source": [
    "- This section is required to train the **already tuned** model and obtain the test predictions (or prediction probabilities) with it.\n",
    "- As written in the instructions, your code must not have any runtime issues, so **do NOT include your grid search here!** You will still need to tune your model to pass the thresholds. However, you need to keep that as your personal work and should NOT include the grid search here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fda9ead-1c99-4749-9c29-f561b0e8d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=12, criterion='entropy', max_depth=13, max_leaf_nodes=94, min_samples_leaf=5, min_samples_split=5)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict_proba(X_test)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2d1c1-edd2-428c-9944-2bafc8f9291b",
   "metadata": {},
   "source": [
    "## 4) Exporting the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e92cdc-e3e0-423a-a248-cf3b1cf99f68",
   "metadata": {},
   "source": [
    "Include the code that (1) puts the predictions in the format that Kaggle understands and (2) exports it as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1f881e2-bccf-49e3-aa6c-ff4381883031",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,        \n",
    "    'predicted': y_preds   \n",
    "})\n",
    "\n",
    "submission.to_csv('FINAL_CLASS_SUB.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7a69d-7d88-4750-a6a8-b902a32af833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
